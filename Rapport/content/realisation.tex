\sectionnn{Réalisation}
Comme mentionné dans l’état de l’art la visualisation des grands jeux de donnée souffre de cette \textit{malédicion de la dimensionalité}. En effet, le problème des projections n’est pas celui de projeter les données, mais de les projeter fidèlement. 
C’est-à-dire, la difficulté pour la méthode à respecter le rapport des distances des points entre le jeu de donné original (à n-dimensions) et la projection finale (qui réduit ce dernier à deux ou 3 dimensions). 
Plus le nombre de dimensions augmente, plus il devient compliqué de respecter les distances après réduction. 
\newline
Le choix d’une méthode de projection par rapport à une autre dépend d'autant plus de la sémantique et de la structure des données. Ce choix dépendra aussi de ce que l’on souhaite observer : voulons-nous mettre en valeur les classes ? Voulons-nous détecter des valeurs aberrantes? 
En fonction de cela, le choix de la méthode et de son paramétrage peut grandement différer.
\smallskip
Une fois le jeu de données projeté, il faut s’assurer de la qualité de cette projection.
Pour cela nous disposons de nombreux critères qui peuvent donner un indice sur le clustering, outlying et la densité de la projection. Certains critères sont même propres à certaines méthodes(comme le projection score pour l'ACP).
Encore une fois, en fonction de ce que nous voulons observer certains critères seront plus utiles que d'autres. De plus, ces critères ne font pas tout : d’une certaine façon, une projection se doit d’être à la fois la plus compréhensible par l’Homme et la plus fidèle possible au jeu de données qu’elle représente.


\medskip
C'est dans ce but, que dans la partie réalisation de cette UE, nous avons proposé une méthode d'évaluation des différentes méthodes de projections citées.
Le but de cette démarche est de proposer un outil capable de comparer sa méthode de projection à d'autres via un tableau de scoring pour se rendre compte de la pertinence et de la fidélité des différentes méthodes de projection.
Dans cette partie nous détaillerons l'implémentation, les jeux de données utilisés et le fonctionnement du programme.
\medskip


\section{Implémentation}

\subsection{Méthodologie} 
Nous avons donc mis en place un programme avec un système de score qui classe l’efficacité des différentes méthodes de projection pour un même jeu de données. 
C’est-à-dire que le jeu de donnée va être projeté une fois par chaque méthode, puis le résultat sera évalué.
Pour mesurer ces résultats nous avons utilisé différents critères issus de la librairie R \textit{clusterCrit}\cite{desgraupes2018package} et \textit{scikit-learn}\cite{scikitpdf}.
ClusterCrit est une librairie R qui fournit une liste de critéres permettant d'attester de la qualité interne des clusters et une liste de critères qui permet 
de mesurer la similarité entre deux partitions. Ces critères prennent seulement en compte la répartition des points dans les différents cluster et ne permettent pas de 
mesurer la qualité de la distribution \cite{desgraupes2013clustering}

\subsection{Jeux de données utilisés}
Pour comparer les différentes méthodes nous avons générés plusieurs jeux de donnés qui présentent les caractéristiques suivantes :
\begin{itemize}
    \item Un jeu de données avec deux clusters superposés avec un seul(Figure 5)
    \item Un jeu de données avec trois clusters bien distincts(Figure 6)
    \item Un jeu de données avec trois clusters superposés(Figure 7)
\end{itemize}

Pour les créer nous avons utilisé la méthode \textit{makeblobs} de la librairie \textit{sklearn}. 

Les paramètres fixes sont :
\begin{itemize}
    \item \textit{n samples} qui représente le nombre de points total qui sera également réparti entre les différents clusters. Nous l'avons fixé à vingt-mille.
    \item \textit{n features} qui représente le nombre de variables pour chaque échantillon. Nous l'avons fixé à vingt-et-un.
\end{itemize}
Nous avons fait varier les paramètres suivants pour obtenir la répartition souhaitée : 
\begin{itemize}
    \item \textit{centers} qui détermine le nombre de centres à génerer
    \item \textit{cluster std} qui détermine l'écart type des clusters
    \item \textit{random state} qui détermine un nombre aléatoire pour la crétion du jeu de données.
\end{itemize}


\begin{center}
    \begin{figure}[ht!]
        \centering
        
        \includegraphics[width=6cm, keepaspectratio]{imports/three_distincts.png}
        
        \caption{Trois clusters distincts}
    \end{figure}
\end{center}

\begin{center}
    \begin{figure}[ht!]
        \centering
        
        \includegraphics[width=6cm, keepaspectratio]{imports/three_overlaped.png}
        
        \caption{Trois clusters avec chevauchement}
    \end{figure}
\end{center}


\begin{center}
    \begin{figure}[ht!]
        \centering
        
        \includegraphics[width=6cm, keepaspectratio]{imports/two_overlaped_one_alone.png}
        
        \caption{Deux clusters avec chevauchement et un cluster seul}
    \end{figure}
\end{center}


\subsection{Fonctionnement du programme}
La librairie \textit{rpy2} est utilisée pour pouvoir charger et utiliser des librairies R sous Python. Comme précisé plus haut
nous appliquons cela à la librairie \textit{clusterCrit}. Et appelons tous les critéres d'intérêts qui sont listés dans la variable globale \textit{crit}.

La fonction \textit{calculate} va utiliser chaque méthode de réduction sur le jeu de donnée, le resultat de cette réduction sera stockée dans
la variable \textit{p2} à laquelle on appliquera la fonction \textit{IntVector} qui permet de convertir un tableau python en vecteur utilisable sous R. 
Comme éléments de comparaison, on applique Kmean sur le jeu de donnée originel. ce qui permet de partitionner le jeu de données en k groupes. 
Dans notre cas k = 3.

Une fois ces deux opérations effectuées, il y a deux variables \textit{rds} (qui représente le résultat de la méthode de réduction), et \textit{rdsOriginal} (qui représente le jeu de
donnée original après kmean.) Pour comparer, ces deux jeux de données nous les passons en paramètre de la fonction \textit{metricsCalcul}.
Celle-ci va itérer à travers tous les critèrs de la liste \textit{crit} et appliquer l'évaluation de chaque critères.

\subsection{Expérimentation}
Les résultats seront stockés sous la forme d'un tableau comprenant les résultats de chaque méthode pour chaque jeux de données. 
Voici un tableau contenant la valeur minimum et Maximum de chaque critères.

\begin{center}
    \begin{figure}[!ht] 
 \resizebox{\textwidth}{!}{\begin{tabular}{ |*{15}{c|}}
                    \hline
                    & Czekanowski& Folkes& Hubert & Jaccard & Kulczynski & McNemar & Phi & Precision & Rand & Recall & Rogers Tanimoto & Russel Rao & SokalS1 & SokalS2\\ \hline
                    Min & 0 & 0 & -1 & 0 & 0 & 0 & -1 & 0 & 0 & 0 & 0 & 0 &0 & 0\\ \hline
                    Max & 1 & 1 & 1 & 1 & 1 & 1 & $\infty$ &1 &1 &1 &1 &1  &1 &1\\ 
                    \hline  
                \end{tabular}}
        \caption{Résultats des jeux de données pour l'indice de Hubert}
    \end{figure}

\end{center}


\section{Analyse des résultats}

\begin{center}
    \begin{figure}[!ht] 
 \resizebox{\textwidth}{!}{\begin{tabular}{ |*{16}{c|}}
                    \hline
                    & Czekanowski& Folkes& Hubert & Jaccard & Kulczynski & McNemar & Phi & Precision & Rand & Recall & Rogers Tanimoto & Russel Rao & SokalS1 & SokalS2 & Total score\\ \hline
                    MDS dataset1 & 0.12	&0.16&-0.0&0.06 &0.2&306.01& 0.0 &0.33&0.64	&0.07&0.47&0.02&0.03&0.78&  \textbf{0/14} \\ \hline
                    tSNE dataset1 &	0.02	&0.06	&0.0	&0.01	&0.17	& \textbf{394.03} &	0.0&	\textbf{0.34}&	\textbf{0.66}&	0.01&	\textbf{0.5}&	0.0&	0.01	&\textbf{0.8} & \textbf{5/14} \\ \hline
                    pca dataset1 &	\textbf{0.31}&	\textbf{0.31}&	-0.0&	\textbf{0.18}	&\textbf{0.31}&	55.63&	-0.0&	0.33&	0.57&	\textbf{0.28}&	0.4&	\textbf{0.1}&	\textbf{0.1}&	0.73& \textbf{7/14} \\ \hline
                    mds dataset2 	&0.08&	0.12&	-0.0&	0.04&	0.19&	344.52&	0.0&	0.33&	0.65&	0.04&	0.48&	0.01&	0.02&	0.79& \textbf{0/14} \\ \hline
                    tSNE dataset2 &	0.02	&0.06	&0.0&	0.01&	0.17&	\textbf{393.52} &	0.0&	\textbf{0.34}&	\textbf{0.66}&	0.01&	\textbf{0.5}&	0.0&	0.0&	\textbf{0.8}& \textbf{5/14} \\ \hline
                    pca dataset2 &	\textbf{0.13}	&\textbf{0.17}	&0.0	&\textbf{0.07}&	\textbf{0.21}&	293.66&	0.0&	0.33&	0.64&	\textbf{0.08}&	0.47&	\textbf{0.03} & \textbf{0.04}&	0.78& \textbf{7/14} \\ \hline
                    mds dataset3 &	0.04&	0.08&	-0.0&	0.02&	\textbf{0.18}&	377.4&	0.0&	0.33&	\textbf{0.66}&	0.02&	0.49&	0.01&	0.01&	\textbf{0.8}& \textbf{3/14} \\ \hline
                    tSNE dataset3 &	0.02&	0.06&	0.0&	0.01&	\textbf{0.18}&	\textbf{391.69} &	0.0&	\textbf{0.35} &	\textbf{0.66}&	0.01&	\textbf{0.5}&	0.0&	0.01&	\textbf{0.8}& \textbf{5/14} \\ \hline
                    pca dataset3 & \textbf{0.06} &	\textbf{0.1}&	-0.0&	\textbf{0.03}&	\textbf{0.18}&	362.47&	-0.0&	0.33&	\textbf{0.66} &	\textbf{0.03}&	0.49&	0.01&	\textbf{0.02}&	0.79& \textbf{7/14} \\
                    \hline
                \end{tabular}}
        \caption{Résultats des jeux de données pour l'indice de Hubert}
    \end{figure}

\end{center}


\subsection{Analyse de chaques critères}
Dans cette section, nous allons détailler les résultats des jeux de données pour les différentes indices.

\subsubsection{L'index de Czekanowski Dice}
On peut voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.

\subsubsection{L'index de Folkes Mallow}
On peut voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.

\subsubsection{L'indice de Hubert}
Pour cet indice les résultats précis sont les suivants : 
\smallskip

\begin{center}
    \begin{figure}[!ht]  
        \begin{tabular}{ | l | c | }
            \hline			
            MDS dataset1 & -0.000366 \\ \hline
            tSNE dataset1 & 0.000334 \\ \hline
            PCA dataset1 & 0.000119  \\ \hline
            MDS dataset2 & -0.001389 \\  \hline
            tSNE dataset2 & 0.000749 \\ \hline
            PCA dataset2 & 0.00416  \\ \hline
            MDS dataset3 & -0.000556 \\ \hline
            tSNE dataset3 & 0.001285 \\ \hline
            PCA dataset3 & -0.000369  \\ 
            \hline  
        \end{tabular}
    \caption{Résultats des jeux de données pour l'indice de Hubert}
    \end{figure}
\end{center}
\smallskip

Nous pouvons voir que les valeurs $0.0$ et $-0.0$ viennent de l'arrondi des résultats. Les résultats à cet indice sont négligeables la corrélation est donc proche de zéro.


\subsubsection{L'indice de Jaccard}
On peut voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.

\subsubsection{L'indice de Kulczynski}
On peut voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.
On peut noter que pour le troisième jeu de données (trois clusters se chevauchant) toutes les méthodes de réduction ont la même efficacité : 
elles ont toutes les trois eu un score de 0.18.


\subsubsection{L'indice de McNemar}
On peut voir que pour les trois jeux de données c'est le tSNE qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.
Néanmoins, son score(394.03) est très proche du second jeux de données(393.52).

\subsubsection{L'indice de Phi}

Pour cet indice les résultats précis sont les suivants : 
\smallskip

\begin{center}
    \begin{figure}[!ht]  
        \begin{tabular}{ | l | c | }
            \hline			
            MDS dataset1 & 9.172429e-12 \\ \hline
            tSNE dataset1 & 3.508708e-12 \\ \hline
            PCA dataset1 & -2.322457e-12  \\ \hline
            MDS dataset2 & -0.001389 \\  \hline
            tSNE dataset2 & 6.570988e-12 \\ \hline
            PCA dataset2 & 4.064610e-12 \\ \hline
            MDS dataset3 & -3.519841e-12 \\ \hline
            tSNE dataset3 & 1.093924e-11 \\ \hline
            PCA dataset3 & -2.284271e-12  \\ 
            \hline  
        \end{tabular}
    \caption{Résultats des jeux de données pour l'indice Phi}
    \end{figure}
\end{center}
\smallskip

Nous pouvons voir que les valeurs $0.0$ et $-0.0$ viennent de l'arrondi des résultats. Les résultats à cet indice ont pour ordre de grandeur $10^{-12}$, ils sont
donc négligeables.


\subsubsection{L'indice de Jaccard}
On peut voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve de meilleur résultat.

\subsubsection{L'indice Précision}
On peut voir que pour cet indice c'est le tSNE qui a les meilleurs résultats pour les trois jeux de données.
Cependant la différence reste faible par rapport aux autre méthodes (seulement 0.01).

\subsubsection{L'indice Rand}
Pour le premier et le second jeu de données le tSNE a un meilleur score que les deux autres techniques.
Pour le troisième jeu de données, toutes les techniques semblent se valoir : elles ont tous le même score de 0.66.


\subsubsection{L'indice Recall}
Pour cet indice c'est l'ACP qui obtient de meilleurs scores pour les trois jeux de données.
On peut noter que la différence de score entre les méthodes est beaucoup plus marquée pour le premier jeu de données.


\subsubsection{L'indice Rogers Tanimoto}
Pour cet indice c'est principalement toutes les techniques ont sensiblement le même score. Néanmoins on peut noter une légère tendance 
pour la tSNE a être plus efficace. Elle est de 0.5 pour les trois jeux de données.

\subsubsection{L'indice Russel Rao}
Pour cet indice, nous pouvons voir que pour les trois jeux de données c'est l'ACP qui a les meilleurs résultats pour les trois jeux de données.
Parmis les trois jeux de données c'est le premier (deux clusters se chevauchant et un seul) avec lequel on retrouve une différence plus marquées entre 
les scores des méthodes : le score de l'ACP est à 0.1 tandis que le score du mds est à 0.02 et celui du tSNE est à 0.0.

\subsubsection{Les indices Sokal Sneath}

Pour le premier indice de Sokal Sneath nous pouvons voir que c'est l'ACP qui a de meilleurs score pour tous les jeux de données, et que parmis ceux-ci
c'est avec le premier que l'on trouve la plus grande différence de score entre méthode : le score de la PCA est à 0.1 tandis que le score du mds est à 0.03 et celui du tSNE est à 0.1.

Pour le second indice de Sokal Sneath c'est le tSNE qui semble avoir le meilleur score pour les trois jeux de donnéeS.
On peut noter que pour le troisième jeu de données, les scores du MDS, du tSNE sont les mêmes 0.8 et que celle de l'ACP s'en rapproche de très près étant donné que sa valeur est de 0.79.

\subsection{Discussion des résultats}
Nous pouvons constater que dans une grande majorité des cas c'est l'ACP qui a les meilleurs résultats. De plus, c'est systématiquement le premier jeu de données
(deux clusters qui se chevauchent et un cluster seul) pour lequel cette méthode fonctionne le mieux.
Néanmoins ces résultats ne permettent pas d'affirmer que l'ACP est la méthode de projection la plus efficace, et ce, pour plusieurs raisons.
D'une part il serait judicieux d'ajouter à ce programme d'autre techniques de projection, ce qui permet d'avoir plus d'élements de comparaison.
Tout d'abord, il aurait été intéressant de tester la méthode classNerv(qu'il a été compliqué d'impléter suite à des problèmes de compatibilité), qui au vu des résultats présentés dans la première partie de ce rapport
semble être particulièrement performante et aurait pu montrer des résultats plus probants que ceux de l'ACP. 

De plus, il faudrait diversifier les différents jeux de données c'est à dire faire des tests sur d'autres jeux de données générés , par exemple
un jeu de données avec trois clusters bien distincts et des outliers, ou bien un jeu de données avec un clusters et plusieurs outliers.
Il faudrait également tester le programme avec des jeux de données qui proviennent de sources réelles (par exemple trouvables sur Kaggle ou Datahub.io) et tester différents types
de jeux de donné (bivariés, multivariés, catégoriels...)


\section{Conclusion}
Pour conclure, nous avons vu que dans une certaine mesure l'ACP se révèle être la méthode la plus efficace pour trois types de jeux de données générés différents. À savoir :
\begin{itemize}
    \item Un jeu de données avec deux clusters superposés avec un seul(Figure 5)
    \item Un jeu de données avec trois clusters bien distincts(Figure 6)
    \item Un jeu de données avec trois clusters superposés(Figure 7)
\end{itemize}

Il serait intéressant d'utiliser ClassNerV qui en théorie est la plus performante. Nous pourrions par exemple 
effectuer un étalonnage des différentes autres méthodes de projection, en l'utilisant en tant que référentiel. 
C'est à dire que lorsqu'un jeu de donnée serait projeté avec une autre méthode de projection, le résultat obtenu serait comparé à la projection de ce même jeu de donnée obtenue via ClassNerV.
Puis, en fonction des divergences entre ces deux jeux de données projetés, certains biais seraient mesurés et affichés.
De plus dans l'objectif d'être le plus précis possible, nous envisageons également de mesurer les différents critères de la projection(clustering, outlying, forme...)et de croiser les résultats obtenus à ceux de la comparaison des deux projections.
Nous pensons aussi qu'il serait judicieux de mettre en place un système de "scoring", qui permettrait de rendre plus intuitive l'appréhension des résultats relatifs à la qualité de la visualisation. 

